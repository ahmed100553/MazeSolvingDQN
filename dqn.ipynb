{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0W4f48tDKJh-",
        "outputId": "506a3040-dc74-44a5-ff50-a51780adaff5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\Programming\\Projects\\TrustRegionECE5523\\.conda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 0 completed with 4203 steps, average loss 0.08116441433481489 and total reward -2483.5\n",
            "Episode 1 completed with 233 steps, average loss 3.221300187802478 and total reward -80.5\n",
            "Episode 2 completed with 459 steps, average loss 2.9100302903221063 and total reward -298.5\n",
            "Episode 3 completed with 1442 steps, average loss 5.422208903339302 and total reward -1052.5\n",
            "Episode 4 completed with 264 steps, average loss 2.9701837592504243 and total reward -129.5\n",
            "Episode 5 completed with 714 steps, average loss 2.663781963226174 and total reward -570.5\n",
            "Episode 6 completed with 803 steps, average loss 3.0974252533496984 and total reward -616.5\n",
            "Episode 7 completed with 229 steps, average loss 2.925141582322433 and total reward -110.5\n",
            "Episode 8 completed with 68 steps, average loss 3.5062300931004917 and total reward 46.5\n",
            "Episode 9 completed with 636 steps, average loss 3.395953525630933 and total reward -492.5\n",
            "Episode 10 completed with 683 steps, average loss 3.851654521193721 and total reward -517.5\n",
            "Episode 11 completed with 555 steps, average loss 5.447823016600566 and total reward -409.5\n",
            "Episode 12 completed with 8996 steps, average loss 69463100.87663072 and total reward -8525.5\n",
            "Episode 13 completed with 17072 steps, average loss 6019145.568167292 and total reward -16027.5\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 92\u001b[0m\n\u001b[0;32m     90\u001b[0m next_state, reward, done \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     91\u001b[0m agent\u001b[38;5;241m.\u001b[39mstore_transition(state, action, reward, next_state, done)\n\u001b[1;32m---> 92\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m cumulative_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[0;32m     94\u001b[0m episode_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
            "File \u001b[1;32me:\\Programming\\Projects\\TrustRegionECE5523\\DQNAgent.py:78\u001b[0m, in \u001b[0;36mDQNAgent.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     77\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 78\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_update_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "File \u001b[1;32me:\\Programming\\Projects\\TrustRegionECE5523\\.conda\\Lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m             )\n\u001b[1;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
            "File \u001b[1;32me:\\Programming\\Projects\\TrustRegionECE5523\\.conda\\Lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
            "File \u001b[1;32me:\\Programming\\Projects\\TrustRegionECE5523\\.conda\\Lib\\site-packages\\torch\\optim\\adam.py:223\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    211\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    213\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    214\u001b[0m         group,\n\u001b[0;32m    215\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    220\u001b[0m         state_steps,\n\u001b[0;32m    221\u001b[0m     )\n\u001b[1;32m--> 223\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
            "File \u001b[1;32me:\\Programming\\Projects\\TrustRegionECE5523\\.conda\\Lib\\site-packages\\torch\\optim\\optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32me:\\Programming\\Projects\\TrustRegionECE5523\\.conda\\Lib\\site-packages\\torch\\optim\\adam.py:784\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    782\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 784\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32me:\\Programming\\Projects\\TrustRegionECE5523\\.conda\\Lib\\site-packages\\torch\\optim\\adam.py:414\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    412\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom)\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 414\u001b[0m     step \u001b[38;5;241m=\u001b[39m \u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    416\u001b[0m     bias_correction1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mstep\n\u001b[0;32m    417\u001b[0m     bias_correction2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mstep\n",
            "File \u001b[1;32me:\\Programming\\Projects\\TrustRegionECE5523\\.conda\\Lib\\site-packages\\torch\\optim\\optimizer.py:106\u001b[0m, in \u001b[0;36m_get_value\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m x\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt  # Add this import\n",
        "from maze import Maze\n",
        "from DQNAgent import DQNAgent\n",
        "from rl_evaluator import RLEvaluator, run_comparison  # Add this import\n",
        "import torch\n",
        "\n",
        "# Constants\n",
        "GAME_HEIGHT = 125\n",
        "GAME_WIDTH = 125\n",
        "NUMBER_OF_TILES = 9\n",
        "SCREEN_HEIGHT = 700\n",
        "SCREEN_WIDTH = 700\n",
        "TILE_SIZE = GAME_HEIGHT // NUMBER_OF_TILES\n",
        "\n",
        "# Maze layout\n",
        "level = [\n",
        "    \"XXXXXXXXXXXXX\",\n",
        "    \"X           X\",\n",
        "    \"X XXX X XXX X\",\n",
        "    \"X   X X   X X\",\n",
        "    \"XXX X XXX X X\",\n",
        "    \"X   X   X   X\",\n",
        "    \"X XXX XXX X X\",\n",
        "    \"X X   X   X X\",\n",
        "    \"X XXX X XXX X\",\n",
        "    \"X   X X   X X\",\n",
        "    \"XXX XXX XXX X\",\n",
        "    \"XP        X X\",\n",
        "    \"XXXXXXXXXXXXX\",\n",
        "]\n",
        "\n",
        "env = Maze(level, goal_pos=(1, 5), MAZE_HEIGHT=GAME_HEIGHT, MAZE_WIDTH=GAME_WIDTH, SIZE=TILE_SIZE)\n",
        "\n",
        "# Define state and action dimensions based on your environment's needs\n",
        "state_dim = 2  # Assuming the state is a 2D coordinate (row, col)\n",
        "action_dim = 4  # Four actions: left, up, right, down\n",
        "agent = DQNAgent(state_dim, action_dim)\n",
        "\n",
        "# Add evaluation function\n",
        "def evaluate_agent(env, agent, num_runs=5):\n",
        "    total_reward = 0\n",
        "    for _ in range(num_runs):\n",
        "        state = env.reset_state()\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        while not done:\n",
        "            action = agent.get_action(state, epsilon=0.0)  # No exploration\n",
        "            next_state, reward, done = env.step(action)\n",
        "            episode_reward += reward\n",
        "            state = next_state\n",
        "        total_reward += episode_reward\n",
        "    average_reward = total_reward / num_runs\n",
        "    return average_reward\n",
        "\n",
        "\n",
        "def plot_path(agent_path, episode, max_steps=100):\n",
        "    grid = np.zeros((env.number_of_tiles, env.number_of_tiles))\n",
        "    for (row, col) in env.walls:\n",
        "        grid[row, col] = -1  # Represent walls with -1\n",
        "    for step, (row, col) in enumerate(agent_path[:max_steps]):\n",
        "        grid[row, col] = step + 1  # Mark path with step number\n",
        "    grid[env.goal_pos] = 10  # Mark goal with 10\n",
        "    plt.figure(figsize=(env.number_of_tiles, env.number_of_tiles))  # Adjust figure size to match maze size\n",
        "    plt.imshow(grid, cmap=\"viridis\", origin=\"upper\")\n",
        "    plt.colorbar(label=\"Steps (0=start, 10=goal)\")\n",
        "    plt.title(f\"Path Taken by Agent - Episode {episode}\")\n",
        "    plt.show()\n",
        "\n",
        "# Initialize lists to store rewards and losses\n",
        "episode_rewards = []\n",
        "episode_losses = []\n",
        "\n",
        "# Training loop\n",
        "num_episodes = 100\n",
        "evaluation_interval = 50  # Evaluate every 50 episodes\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset_state()\n",
        "    done = False\n",
        "    steps = 0\n",
        "    cumulative_loss = 0\n",
        "    episode_reward = 0\n",
        "    agent_path = [state]  # Track path for visualization\n",
        "\n",
        "    while not done:\n",
        "        action = agent.get_action(state)\n",
        "        next_state, reward, done = env.step(action)\n",
        "        agent.store_transition(state, action, reward, next_state, done)\n",
        "        loss = agent.train()\n",
        "        cumulative_loss += loss\n",
        "        episode_reward += reward\n",
        "        steps += 1\n",
        "        state = next_state\n",
        "        agent_path.append(state)  # Add to path\n",
        "\n",
        "    avg_loss = cumulative_loss / steps if steps > 0 else 0\n",
        "    episode_rewards.append(episode_reward)  # Store total reward\n",
        "    episode_losses.append(avg_loss)         # Store average loss\n",
        "\n",
        "    # Print episode details\n",
        "    print(f\"Episode {episode} completed with {steps} steps, average loss {avg_loss} and total reward {episode_reward}\")\n",
        "\n",
        "    # Evaluate the agent and plot path every 50 episodes\n",
        "    #if episode % evaluation_interval == 0 and episode != 0:\n",
        "        #avg_reward = evaluate_agent(env, agent)\n",
        "        #print(f\"Evaluation after episode {episode}: Average Reward = {avg_reward}\")\n",
        "    if episode_reward > 50:\n",
        "        plot_path(agent_path, episode)\n",
        "\n",
        "# After training, plot rewards and losses\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(episode_rewards)\n",
        "plt.title('Episode Rewards')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(episode_losses)\n",
        "plt.title('Average Loss per Episode')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmmLmIOvKJiB",
        "outputId": "3c324788-02fc-4210-ac25-c231c35fea8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(11, 1), (11, 2), (11, 1), (11, 2), (11, 2), (11, 1), (11, 2), (11, 3), (10, 3), (9, 3), (9, 3), (9, 3), (9, 3), (10, 3), (9, 3), (9, 3), (9, 3), (9, 3), (9, 3), (9, 3), (9, 3), (9, 3), (9, 3), (9, 3), (9, 3), (9, 3), (9, 3), (9, 3), (9, 3), (9, 3), (9, 3), (9, 3), (9, 3), (9, 3), (9, 3), (9, 3), (9, 3), (9, 2), (9, 1), (8, 1), (7, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (5, 1), (5, 2), (5, 3), (4, 3), (3, 3), (3, 2), (3, 1), (2, 1), (1, 1), (1, 2), (1, 3), (1, 4), (1, 4), (1, 4), (1, 5)]\n"
          ]
        }
      ],
      "source": [
        "print(agent_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCW0vo42KJiB",
        "outputId": "d4b342dc-5f4b-4583-bea8-46068bb2a52e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pygame 2.5.0 (SDL 2.28.0, Python 3.11.10)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
            "MoviePy - Building video episode_50.mp4.\n",
            "MoviePy - Writing video episode_50.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MoviePy - Done !\n",
            "MoviePy - video ready episode_50.mp4\n"
          ]
        }
      ],
      "source": [
        "import pygame\n",
        "import os\n",
        "from moviepy import ImageSequenceClip\n",
        "\n",
        "# Initialize pygame\n",
        "pygame.init()\n",
        "\n",
        "# Constants for pygame visualization\n",
        "SCREEN_WIDTH = 700\n",
        "SCREEN_HEIGHT = 700\n",
        "TILE_SIZE = SCREEN_WIDTH // env.number_of_tiles\n",
        "FPS = 5  # Frames per second for animation\n",
        "\n",
        "# Colors\n",
        "BLACK = (0, 0, 0)\n",
        "WHITE = (255, 255, 255)\n",
        "GRAY = (200, 200, 200)\n",
        "BLUE = (0, 0, 255)\n",
        "RED = (255, 0, 0)\n",
        "GREEN = (0, 255, 0)\n",
        "\n",
        "# Create screen\n",
        "screen = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT))\n",
        "pygame.display.set_caption(\"Agent Path Animation\")\n",
        "\n",
        "# Function to draw the maze\n",
        "def draw_maze(env, agent_position, path=[]):\n",
        "    screen.fill(BLACK)\n",
        "    for row in range(env.number_of_tiles):\n",
        "        for col in range(env.number_of_tiles):\n",
        "            rect = pygame.Rect(col * TILE_SIZE, row * TILE_SIZE, TILE_SIZE, TILE_SIZE)\n",
        "            if (row, col) in env.walls:\n",
        "                pygame.draw.rect(screen, GRAY, rect)  # Walls\n",
        "            elif (row, col) == env.goal_pos:\n",
        "                pygame.draw.rect(screen, GREEN, rect)  # Goal\n",
        "            elif (row, col) in path:\n",
        "                pygame.draw.rect(screen, BLUE, rect)  # Path\n",
        "            else:\n",
        "                pygame.draw.rect(screen, WHITE, rect)  # Empty space\n",
        "            pygame.draw.rect(screen, BLACK, rect, 1)  # Grid lines\n",
        "\n",
        "    # Draw agent\n",
        "    agent_rect = pygame.Rect(agent_position[1] * TILE_SIZE, agent_position[0] * TILE_SIZE, TILE_SIZE, TILE_SIZE)\n",
        "    pygame.draw.rect(screen, RED, agent_rect)\n",
        "\n",
        "# Export animation\n",
        "def export_animation(agent_path, filename=\"episode_50.mp4\"):\n",
        "    clock = pygame.time.Clock()\n",
        "    running = True\n",
        "    frame_count = 0\n",
        "    frame_paths = []\n",
        "\n",
        "    for step, position in enumerate(agent_path):\n",
        "        if not running:\n",
        "            break\n",
        "        for event in pygame.event.get():\n",
        "            if event.type == pygame.QUIT:\n",
        "                running = False\n",
        "\n",
        "        draw_maze(env, position, path=agent_path[:step])\n",
        "        pygame.display.flip()\n",
        "\n",
        "        # Save frames for animation\n",
        "        frame_count += 1\n",
        "        frame_path = f\"frame_{frame_count}.png\"\n",
        "        pygame.image.save(screen, frame_path)\n",
        "        frame_paths.append(frame_path)\n",
        "\n",
        "        clock.tick(FPS)\n",
        "\n",
        "    pygame.quit()\n",
        "\n",
        "    # Create video from frames using moviepy\n",
        "    clip = ImageSequenceClip(frame_paths, fps=FPS)\n",
        "    clip.write_videofile(filename, fps=FPS)\n",
        "\n",
        "    # Clean up frame files\n",
        "    for frame_path in frame_paths:\n",
        "        os.remove(frame_path)\n",
        "\n",
        "# Example usage: Export animation for episode 50\n",
        "# Assuming agent_path_50 is the path of the agent in episode 50\n",
        "export_animation(agent_path, filename=\"episode_50.mp4\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gY4LvI5pKJiB",
        "outputId": "050f14d9-ea26-47d7-8e7c-79ed8c771b68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "434516270666000\n",
            "2147483648\n"
          ]
        }
      ],
      "source": [
        "current_seed = torch.initial_seed()\n",
        "print(current_seed)\n",
        "print(np.random.get_state()[1][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "from maze import Maze\n",
        "\n",
        "class replay_buffer(object):\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = deque(maxlen=self.capacity)\n",
        "\n",
        "    def store(self, observation, action, reward, next_observation, done):\n",
        "        observation = np.expand_dims(observation, 0)\n",
        "        next_observation = np.expand_dims(next_observation, 0)\n",
        "\n",
        "        self.memory.append([observation, action, reward, next_observation, done])\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.memory, batch_size)\n",
        "        observation, action, reward, next_observation, done = zip(* batch)\n",
        "\n",
        "        return np.concatenate(observation, 0), action, reward, np.concatenate(next_observation, 0), done\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "\n",
        "class NoisyLinear(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, std_init=0.4):\n",
        "        super(NoisyLinear, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.std_init = std_init\n",
        "\n",
        "        self.weight_mu = nn.Parameter(torch.FloatTensor(self.output_dim, self.input_dim))\n",
        "        self.weight_sigma = nn.Parameter(torch.FloatTensor(self.output_dim, self.input_dim))\n",
        "\n",
        "        self.bias_mu = nn.Parameter(torch.FloatTensor(self.output_dim))\n",
        "        self.bias_sigma = nn.Parameter(torch.FloatTensor(self.output_dim))\n",
        "\n",
        "        self.register_buffer('weight_epsilon', torch.FloatTensor(self.output_dim, self.input_dim))\n",
        "        self.register_buffer('bias_epsilon', torch.FloatTensor(self.output_dim))\n",
        "\n",
        "        self.reset_parameter()\n",
        "        self.reset_noise()\n",
        "\n",
        "    def forward(self, input):\n",
        "        if self.training:\n",
        "            weight = self.weight_mu + self.weight_sigma.mul(self.weight_epsilon)\n",
        "            bias = self.bias_mu + self.bias_sigma.mul(self.bias_epsilon)\n",
        "        else:\n",
        "            weight = self.weight_mu\n",
        "            bias = self.bias_mu\n",
        "        return F.linear(input, weight, bias)\n",
        "\n",
        "    def _scale_noise(self, size):\n",
        "        noise = torch.randn(size)\n",
        "        noise = noise.sign().mul(noise.abs().sqrt())\n",
        "        return noise\n",
        "\n",
        "    def reset_parameter(self):\n",
        "        mu_range = 1.0 / np.sqrt(self.input_dim)\n",
        "\n",
        "        self.weight_mu.detach().uniform_(-mu_range, mu_range)\n",
        "        self.bias_mu.detach().uniform_(-mu_range, mu_range)\n",
        "\n",
        "        self.weight_sigma.detach().fill_(self.std_init / np.sqrt(self.input_dim))\n",
        "        self.bias_sigma.detach().fill_(self.std_init / np.sqrt(self.output_dim))\n",
        "\n",
        "    def reset_noise(self):\n",
        "        epsilon_in = self._scale_noise(self.input_dim)\n",
        "        epsilon_out = self._scale_noise(self.output_dim)\n",
        "\n",
        "        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))\n",
        "        self.bias_epsilon.copy_(self._scale_noise(self.output_dim))\n",
        "\n",
        "\n",
        "class categorical_dqn(nn.Module):\n",
        "    def __init__(self, observation_dim, action_dim, atoms_num, v_min, v_max):\n",
        "        super(categorical_dqn, self).__init__()\n",
        "        self.observation_dim = observation_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.atoms_num = atoms_num\n",
        "        self.v_min = v_min\n",
        "        self.v_max = v_max\n",
        "\n",
        "        self.fc1 = nn.Linear(self.observation_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.noisy1 = NoisyLinear(128, 512)\n",
        "        self.noisy2 = NoisyLinear(512, self.action_dim * self.atoms_num)\n",
        "\n",
        "    def forward(self, observation):\n",
        "        x = self.fc1(observation)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.noisy1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.noisy2(x)\n",
        "        x = F.softmax(x.view(-1, self.atoms_num), 1).view(-1, self.action_dim, self.atoms_num)\n",
        "        return x\n",
        "\n",
        "    def reset_noise(self):\n",
        "        self.noisy1.reset_noise()\n",
        "        self.noisy2.reset_noise()\n",
        "\n",
        "    def act(self, observation, epsilon):\n",
        "        if random.random() > epsilon:\n",
        "            dist = self.forward(observation)\n",
        "            dist = dist.detach()\n",
        "            dist = dist.mul(torch.linspace(v_min, v_max, self.atoms_num))\n",
        "            action = dist.sum(2).max(1)[1].detach()[0].item()\n",
        "        else:\n",
        "            action = random.choice(list(range(self.action_dim)))\n",
        "        return action\n",
        "\n",
        "\n",
        "def projection_distribution(target_model, next_observation, reward, done, v_min, v_max, atoms_num, gamma):\n",
        "    batch_size = next_observation.size(0)\n",
        "    # next_observation: [batch_size, ...]\n",
        "    delta_z = float(v_max - v_min) / (atoms_num - 1)\n",
        "    support = torch.linspace(v_min, v_max, atoms_num)\n",
        "    # support: [atoms_num]\n",
        "\n",
        "    next_dist = target_model.forward(next_observation).detach().mul(support)\n",
        "    # next_dist: [batch_size, action_dim, atoms_num]\n",
        "    next_action = next_dist.sum(2).max(1)[1]\n",
        "    # next_action: [batch_size]\n",
        "    next_action = next_action.unsqueeze(1).unsqueeze(1).expand(batch_size, 1, atoms_num)\n",
        "    # next_action: [batch_size, 1, atoms_num]\n",
        "    next_dist = next_dist.gather(1, next_action).squeeze(1)\n",
        "    # next_dist: [batch_size, atoms_num]\n",
        "\n",
        "    reward = reward.unsqueeze(1).expand_as(next_dist)\n",
        "    done = done.unsqueeze(1).expand_as(next_dist)\n",
        "    support = support.unsqueeze(0).expand_as(next_dist)\n",
        "\n",
        "    Tz = reward + (1 - done) * support * gamma\n",
        "    Tz = Tz.clamp(min=v_min, max=v_max)\n",
        "    b = (Tz - v_min) / delta_z\n",
        "    l = b.floor().long()\n",
        "    u = b.ceil().long()\n",
        "\n",
        "    offset = torch.linspace(0, (batch_size - 1) * atoms_num, batch_size).long().unsqueeze(1).expand_as(next_dist)\n",
        "\n",
        "    proj_dist = torch.zeros_like(next_dist, dtype=torch.float32)\n",
        "    proj_dist.view(-1).index_add_(0, (offset + l).view(-1), (next_dist * (u.float() - b)).view(-1))\n",
        "    proj_dist.view(-1).index_add_(0, (offset + u).view(-1), (next_dist * (b - l.float())).view(-1))\n",
        "    return proj_dist\n",
        "\n",
        "\n",
        "def train(eval_model, target_model, buffer, v_min, v_max, atoms_num, gamma, batch_size, optimizer, count, update_freq):\n",
        "    observation, action, reward, next_observation, done = buffer.sample(batch_size)\n",
        "\n",
        "    observation = torch.FloatTensor(observation)\n",
        "    action = torch.LongTensor(action)\n",
        "    reward = torch.FloatTensor(reward)\n",
        "    next_observation = torch.FloatTensor(next_observation)\n",
        "    done = torch.FloatTensor(done)\n",
        "\n",
        "    proj_dist = projection_distribution(target_model, next_observation, reward, done, v_min, v_max, atoms_num, gamma)\n",
        "\n",
        "    dist = eval_model.forward(observation)\n",
        "    action = action.unsqueeze(1).unsqueeze(1).expand(batch_size, 1, atoms_num)\n",
        "    dist = dist.gather(1, action).squeeze(1)\n",
        "    dist.detach().clamp_(0.01, 0.99)\n",
        "    # * make KL divergence as the loss\n",
        "    loss = - (proj_dist * dist.log()).sum(1).mean()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    eval_model.reset_noise()\n",
        "    target_model.reset_noise()\n",
        "\n",
        "    if count % update_freq == 0:\n",
        "        target_model.load_state_dict(eval_model.state_dict())\n",
        "def evaluate_agent(env, agent, num_runs=5):\n",
        "    total_reward = 0\n",
        "    for _ in range(num_runs):\n",
        "        state = env.reset_state()\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        while not done:\n",
        "            action = agent.act(torch.FloatTensor(np.expand_dims(state, 0)), epsilon=0.0)  # No exploration\n",
        "            next_state, reward, done = env.step(action)\n",
        "            episode_reward += reward\n",
        "            state = next_state\n",
        "        total_reward += episode_reward\n",
        "    average_reward = total_reward / num_runs\n",
        "    return average_reward\n",
        "\n",
        "def plot_path(agent_path, episode):\n",
        "    grid = np.zeros((env.number_of_tiles, env.number_of_tiles))\n",
        "    for (row, col) in env.walls:\n",
        "        grid[row, col] = -1  # Represent walls with -1\n",
        "    for step, (row, col) in enumerate(agent_path):\n",
        "        grid[row, col] = step + 1  # Mark path with step number\n",
        "    grid[env.goal_pos] = 10  # Mark goal with 10\n",
        "    plt.imshow(grid, cmap=\"viridis\", origin=\"upper\")\n",
        "    plt.colorbar(label=\"Steps (0=start, 10=goal)\")\n",
        "    plt.title(f\"Path Taken by Agent - Episode {episode}\")\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    episode = 100\n",
        "    epsilon_init = 0.95\n",
        "    epsilon_decay = 0.95\n",
        "    epsilon_min = 0.01\n",
        "    update_freq = 10\n",
        "    gamma = 0.99\n",
        "    learning_rate = 1e-3\n",
        "    atoms_num = 51\n",
        "    v_min = -100\n",
        "    v_max = 100\n",
        "    batch_size = 64\n",
        "    capacity = 10000\n",
        "    exploration = 10\n",
        "    render = False\n",
        "    GAME_HEIGHT = 125\n",
        "    GAME_WIDTH = 125\n",
        "    NUMBER_OF_TILES = 9\n",
        "    SCREEN_HEIGHT = 700\n",
        "    SCREEN_WIDTH = 700\n",
        "    TILE_SIZE = GAME_HEIGHT // NUMBER_OF_TILES\n",
        "\n",
        "    # Initialize the maze environment\n",
        "    level = [\n",
        "        \"XXXXXXXXXXXXX\",\n",
        "        \"X           X\",\n",
        "        \"X XXX X XXX X\",\n",
        "        \"X   X X   X X\",\n",
        "        \"XXX X XXX X X\",\n",
        "        \"X   X   X   X\",\n",
        "        \"X XXX XXX X X\",\n",
        "        \"X X   X   X X\",\n",
        "        \"X XXX X XXX X\",\n",
        "        \"X   X X   X X\",\n",
        "        \"XXX XXX XXX X\",\n",
        "        \"XP        X X\",\n",
        "        \"XXXXXXXXXXXXX\",\n",
        "    ]\n",
        "    env = Maze(level, goal_pos=(1, 5), MAZE_HEIGHT=GAME_HEIGHT, MAZE_WIDTH=GAME_WIDTH, SIZE=TILE_SIZE)\n",
        "\n",
        "    action_dim = 4  # Four actions: left, up, right, down\n",
        "    observation_dim = 2  # Assuming the state is a 2D coordinate (row, col)\n",
        "    count = 0\n",
        "    target_net = categorical_dqn(observation_dim, action_dim, atoms_num, v_min, v_max)\n",
        "    eval_net = categorical_dqn(observation_dim, action_dim, atoms_num, v_min, v_max)\n",
        "    target_net.load_state_dict(eval_net.state_dict())\n",
        "    optimizer = torch.optim.Adam(eval_net.parameters(), lr=learning_rate)\n",
        "    buffer = replay_buffer(capacity)\n",
        "    weight_reward = None\n",
        "    epsilon = epsilon_init\n",
        "\n",
        "    # Initialize lists to store rewards and losses\n",
        "    episode_rewards = []\n",
        "    episode_losses = []\n",
        "\n",
        "    for i in range(episode):\n",
        "        obs = env.reset_state()\n",
        "        reward_total = 0\n",
        "        steps = 0\n",
        "        cumulative_loss = 0\n",
        "        agent_path = [obs]  # Track path for visualization\n",
        "        if render:\n",
        "            env.render()\n",
        "        while True:\n",
        "            action = eval_net.act(torch.FloatTensor(np.expand_dims(obs, 0)), epsilon)\n",
        "            next_obs, reward, done = env.step(action)\n",
        "            count += 1\n",
        "            if render:\n",
        "                env.render()\n",
        "            buffer.store(obs, action, reward, next_obs, done)\n",
        "            reward_total += reward\n",
        "            obs = next_obs\n",
        "            if i > exploration:\n",
        "                loss = train(eval_net, target_net, buffer, v_min, v_max, atoms_num, gamma, batch_size, optimizer, count, update_freq)\n",
        "                cumulative_loss += loss\n",
        "            steps += 1\n",
        "            agent_path.append(obs)  # Add to path\n",
        "            if done:\n",
        "                if epsilon > epsilon_min:\n",
        "                    epsilon = epsilon * epsilon_decay\n",
        "                if not weight_reward:\n",
        "                    weight_reward = reward_total\n",
        "                else:\n",
        "                    weight_reward = 0.99 * weight_reward + 0.01 * reward_total\n",
        "                avg_loss = cumulative_loss / steps if steps > 0 else 0\n",
        "                episode_rewards.append(reward_total)  # Store total reward\n",
        "                episode_losses.append(avg_loss)       # Store average loss\n",
        "                print(f'episode: {i+1}  reward: {reward_total}  weight_reward: {weight_reward:.3f}  epsilon: {epsilon:.2f}')\n",
        "                break\n",
        "\n",
        "        # Evaluate the agent and plot path every 100 episodes if reward is more than -100\n",
        "        if i % 100 == 0 and i != 0:\n",
        "            avg_reward = evaluate_agent(env, eval_net)\n",
        "            print(f\"Evaluation after episode {i}: Average Reward = {avg_reward}\")\n",
        "            if reward_total > -100:\n",
        "                plot_path(agent_path, i)\n",
        "\n",
        "    # After training, plot rewards and losses\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(episode_rewards)\n",
        "    plt.title('Episode Rewards')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Total Reward')\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(episode_losses)\n",
        "    plt.title('Average Loss per Episode')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "TGgxc-NoKOfl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}